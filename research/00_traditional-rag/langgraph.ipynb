{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42520e2a",
   "metadata": {},
   "source": [
    "# Traditional RAG with LangGraph\n",
    "\n",
    "Welcome to your first hands-on exploration of **Retrieval-Augmented Generation (RAG)** using one of the most popular AI frameworks available today!\n",
    "\n",
    "## üéØ What You'll Learn\n",
    "\n",
    "In this interactive tutorial, we'll build a simple but functional RAG system from scratch using:\n",
    "\n",
    "- **LangChain** for document processing and embeddings\n",
    "- **LangGraph** for orchestrating our RAG workflow\n",
    "- **FastEmbed** for efficient text embeddings\n",
    "- **Claude 3.5 Sonnet** as our language model\n",
    "\n",
    "By the end of this notebook, you'll have a working D&D assistant that can answer questions about character information using semantic search and AI generation.\n",
    "\n",
    "## üîÑ The RAG Process\n",
    "\n",
    "RAG combines two powerful AI techniques:\n",
    "\n",
    "1. **Retrieval**: Finding relevant information from a knowledge base\n",
    "2. **Generation**: Using an LLM to synthesize answers from retrieved context\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986e1a15",
   "metadata": {},
   "source": [
    "## üìö Step 1: Document Indexing\n",
    "\n",
    "The first step in any RAG system is preparing our knowledge base. We need to take our raw text data and transform it into a searchable format.\n",
    "\n",
    "### üîß Text Splitting Strategy\n",
    "\n",
    "For effective retrieval, we need to split our documents into **coherent chunks**. Too large, and we lose precision; too small, and we lose context.\n",
    "\n",
    "We'll use **Markdown Header Text Splitting** because:\n",
    "\n",
    "- ‚úÖ Preserves semantic structure\n",
    "- ‚úÖ Maintains logical document boundaries\n",
    "- ‚úÖ Keeps related information together\n",
    "\n",
    "Let's load our D&D character data and split it into searchable chunks:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a5a8b1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 20 documents.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "file_path = \"characters.md\"\n",
    "\n",
    "# Read the content of the file\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "]\n",
    "\n",
    "# Initialize a text splitter\n",
    "text_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on, strip_headers=False\n",
    ")\n",
    "\n",
    "documents = text_splitter.split_text(text)\n",
    "\n",
    "print(f\"Split into {len(documents)} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e10853",
   "metadata": {},
   "source": [
    "### üß† Creating Semantic Representations\n",
    "\n",
    "To make these text chunks semantically searchable, we need to convert them into **embeddings** - numerical representations that capture meaning.\n",
    "\n",
    "#### How Embeddings Work\n",
    "\n",
    "An embedding model \"compresses\" each text chunk into a high-dimensional vector space (in our case 384 dimensions).\n",
    "Similar concepts end up close together in this space, enabling semantic search.\n",
    "\n",
    "For example:\n",
    "\n",
    "- \"Fighter class abilities\" and \"Warrior combat features\" would have similar embeddings\n",
    "- \"Spellcasting rules\" and \"Magic system\" would cluster together\n",
    "\n",
    "We'll use **FastEmbed** because it is lightweight and fast on CPU. Perfect for experimentation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5220a224",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "\n",
    "embeddings = FastEmbedEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1431daf5",
   "metadata": {},
   "source": [
    "### üóÉÔ∏è Setting Up the Vector Database\n",
    "\n",
    "Now we need a place to store and search our embeddings efficiently.\n",
    "A **vector database** is optimized for similarity search in high-dimensional spaces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87171f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17d4c37",
   "metadata": {},
   "source": [
    "### ‚ö° Indexing the Documents\n",
    "\n",
    "Now for the magic! We'll add our text chunks to the vector store. This process automatically:\n",
    "\n",
    "1. üìù Takes each document chunk\n",
    "2. üî¢ Converts it to an embedding vector\n",
    "3. üíæ Stores both the text and vector for fast retrieval\n",
    "\n",
    "This is where the \"index\" in RAG gets built:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c63ecb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index chunks\n",
    "_ = vector_store.add_documents(documents=documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8625a167",
   "metadata": {},
   "source": [
    "## üîç Step 2: Retrieval & Generation\n",
    "\n",
    "With our knowledge base indexed, we can now build the query-answering pipeline! This involves two key components:\n",
    "\n",
    "1. **üîç Retrieval**: Finding relevant documents based on semantic similarity\n",
    "2. **‚ú® Generation**: Using an LLM to synthesize natural answers from retrieved context\n",
    "\n",
    "### ü§ñ Setting Up the Language Model\n",
    "\n",
    "First, let's initialize our AI model. We'll use **Claude 3.5 Sonnet**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4409f923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "if not os.environ.get(\"ANTHROPIC_API_KEY\"):\n",
    "    os.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass(\"Enter API key for Anthropic: \")\n",
    "\n",
    "llm = init_chat_model(\"claude-3-5-sonnet-latest\", model_provider=\"anthropic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d342b08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lathander\n"
     ]
    }
   ],
   "source": [
    "# Test if the model is working\n",
    "response = llm.invoke(\n",
    "    \"What is the name of the god that is known as the Morninglord in the Forgotten Realms setting? Only respond with the name of the god, nothing else.\"\n",
    ")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274dd77c",
   "metadata": {},
   "source": [
    "### üîó Orchestrating the RAG Pipeline\n",
    "\n",
    "Now for the exciting part - putting it all together! We'll use **LangGraph** to create a workflow that:\n",
    "\n",
    "1. üì• Takes a user question\n",
    "2. üîç Retrieves relevant document chunks\n",
    "3. ü§ñ Generates an answer using the retrieved context\n",
    "4. üì§ Returns the final response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3991cb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "        You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "        Question: {question} \n",
    "        Context: {context} \n",
    "        Answer:\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "\n",
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79153d52",
   "metadata": {},
   "source": [
    "## üß™ Step 3: Testing Our RAG System\n",
    "\n",
    "Time to see our D&D assistant in action! Let's test it with a question about our characters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1f877ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiamat is the five-headed chromatic dragon goddess of evil who rules over Avernus. She appears as both a deity and a powerful endgame boss in various D&D settings including Dragonlance and Forgotten Realms.\n"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\"question\": \"Who is Tiamat?\"})\n",
    "print(response[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dnd-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
